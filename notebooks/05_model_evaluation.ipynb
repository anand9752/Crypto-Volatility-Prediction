{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6e4515",
   "metadata": {},
   "source": [
    "# Model Evaluation for Cryptocurrency Volatility Prediction\n",
    "\n",
    "This notebook focuses on comprehensive evaluation of trained models, including:\n",
    "- Performance metrics analysis\n",
    "- Cross-validation results\n",
    "- Feature importance analysis\n",
    "- Model comparison\n",
    "- Prediction analysis\n",
    "- Business metrics evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98320cc6",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9075f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "import joblib\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "print(\"Model Evaluation Notebook Initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1549fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and models\n",
    "try:\n",
    "    # Load test data\n",
    "    X_test = pd.read_csv('../data/X_test.csv', index_col=0)\n",
    "    y_test = pd.read_csv('../data/y_test.csv', index_col=0)['target']\n",
    "    \n",
    "    # Load trained models\n",
    "    rf_model = joblib.load('../models/random_forest_model.pkl')\n",
    "    gb_model = joblib.load('../models/gradient_boosting_model.pkl')\n",
    "    ensemble_model = joblib.load('../models/ensemble_model.pkl')\n",
    "    \n",
    "    print(f\"Test data loaded: X_test {X_test.shape}, y_test {y_test.shape}\")\n",
    "    print(f\"Models loaded successfully!\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Could not load required files: {e}\")\n",
    "    print(\"Please ensure model training has been completed first.\")\n",
    "    \n",
    "    # Create dummy data for demonstration\n",
    "    print(\"Creating dummy data for demonstration...\")\n",
    "    np.random.seed(42)\n",
    "    X_test = pd.DataFrame(np.random.randn(1000, 50), \n",
    "                         columns=[f'feature_{i}' for i in range(50)])\n",
    "    y_test = pd.Series(np.random.rand(1000) * 0.1, name='target')\n",
    "    \n",
    "    # Create dummy models\n",
    "    rf_model = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=10, random_state=42)\n",
    "    \n",
    "    # Quick training\n",
    "    rf_model.fit(X_test[:800], y_test[:800])\n",
    "    gb_model.fit(X_test[:800], y_test[:800])\n",
    "    \n",
    "    # Use test set for evaluation\n",
    "    X_test = X_test[800:]\n",
    "    y_test = y_test[800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f540c55b",
   "metadata": {},
   "source": [
    "## 1. Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be03471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from all models\n",
    "print(\"=== GENERATING MODEL PREDICTIONS ===\\n\")\n",
    "\n",
    "# Individual model predictions\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "gb_predictions = gb_model.predict(X_test)\n",
    "\n",
    "# Ensemble prediction (weighted average)\n",
    "if 'ensemble_model' in locals():\n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "else:\n",
    "    # Simple ensemble as weighted average\n",
    "    ensemble_predictions = 0.6 * gb_predictions + 0.4 * rf_predictions\n",
    "\n",
    "# Store predictions in DataFrame\n",
    "predictions_df = pd.DataFrame({\n",
    "    'actual': y_test.values,\n",
    "    'rf_pred': rf_predictions,\n",
    "    'gb_pred': gb_predictions,\n",
    "    'ensemble_pred': ensemble_predictions\n",
    "}, index=y_test.index)\n",
    "\n",
    "print(f\"Predictions generated for {len(predictions_df)} samples\")\n",
    "print(f\"\\nPrediction Statistics:\")\n",
    "print(predictions_df.describe())\n",
    "\n",
    "# Display sample predictions\n",
    "print(f\"\\nSample Predictions:\")\n",
    "print(predictions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208d816a",
   "metadata": {},
   "source": [
    "## 2. Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive performance metrics\n",
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    \"\"\"Calculate comprehensive performance metrics.\"\"\"\n",
    "    \n",
    "    # Basic metrics\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    # Additional metrics\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    # Directional accuracy\n",
    "    y_true_diff = np.diff(y_true)\n",
    "    y_pred_diff = np.diff(y_pred)\n",
    "    directional_accuracy = np.mean(np.sign(y_true_diff) == np.sign(y_pred_diff)) * 100\n",
    "    \n",
    "    # Correlation metrics\n",
    "    pearson_corr, _ = pearsonr(y_true, y_pred)\n",
    "    spearman_corr, _ = spearmanr(y_true, y_pred)\n",
    "    \n",
    "    # Residual analysis\n",
    "    residuals = y_true - y_pred\n",
    "    residual_std = np.std(residuals)\n",
    "    residual_skew = stats.skew(residuals)\n",
    "    residual_kurtosis = stats.kurtosis(residuals)\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'R¬≤': r2,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        'Directional Accuracy': directional_accuracy,\n",
    "        'Pearson Correlation': pearson_corr,\n",
    "        'Spearman Correlation': spearman_corr,\n",
    "        'Residual Std': residual_std,\n",
    "        'Residual Skewness': residual_skew,\n",
    "        'Residual Kurtosis': residual_kurtosis\n",
    "    }\n",
    "\n",
    "# Calculate metrics for all models\n",
    "print(\"=== PERFORMANCE METRICS ===\\n\")\n",
    "\n",
    "metrics_list = []\n",
    "metrics_list.append(calculate_metrics(y_test, rf_predictions, 'Random Forest'))\n",
    "metrics_list.append(calculate_metrics(y_test, gb_predictions, 'Gradient Boosting'))\n",
    "metrics_list.append(calculate_metrics(y_test, ensemble_predictions, 'Ensemble'))\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "print(metrics_df.round(4))\n",
    "\n",
    "# Best model identification\n",
    "best_model_r2 = metrics_df.loc[metrics_df['R¬≤'].idxmax(), 'Model']\n",
    "best_model_rmse = metrics_df.loc[metrics_df['RMSE'].idxmin(), 'Model']\n",
    "best_model_mae = metrics_df.loc[metrics_df['MAE'].idxmin(), 'Model']\n",
    "\n",
    "print(f\"\\nüèÜ Best Models by Metric:\")\n",
    "print(f\"   ‚Ä¢ Highest R¬≤: {best_model_r2} ({metrics_df['R¬≤'].max():.4f})\")\n",
    "print(f\"   ‚Ä¢ Lowest RMSE: {best_model_rmse} ({metrics_df['RMSE'].min():.4f})\")\n",
    "print(f\"   ‚Ä¢ Lowest MAE: {best_model_mae} ({metrics_df['MAE'].min():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b6922",
   "metadata": {},
   "source": [
    "## 3. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization of model performance\n",
    "print(\"=== CREATING PERFORMANCE VISUALIZATIONS ===\\n\")\n",
    "\n",
    "# 1. Predicted vs Actual scatter plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Predicted vs Actual Volatility', fontsize=16)\n",
    "\n",
    "models = [('Random Forest', rf_predictions), ('Gradient Boosting', gb_predictions), \n",
    "          ('Ensemble', ensemble_predictions)]\n",
    "\n",
    "for idx, (name, preds) in enumerate(models):\n",
    "    row, col = idx // 2, idx % 2\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(y_test, preds, alpha=0.6, s=20)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_test.min(), preds.min())\n",
    "    max_val = max(y_test.max(), preds.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "    \n",
    "    # Add R¬≤ score\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    ax.text(0.05, 0.95, f'R¬≤ = {r2:.4f}', transform=ax.transAxes, \n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax.set_xlabel('Actual Volatility')\n",
    "    ax.set_ylabel('Predicted Volatility')\n",
    "    ax.set_title(f'{name}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[1, 1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Residual analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Residual Analysis', fontsize=16)\n",
    "\n",
    "for idx, (name, preds) in enumerate(models):\n",
    "    residuals = y_test - preds\n",
    "    \n",
    "    # Residuals vs Predicted\n",
    "    axes[0, idx].scatter(preds, residuals, alpha=0.6, s=20)\n",
    "    axes[0, idx].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0, idx].set_xlabel('Predicted Values')\n",
    "    axes[0, idx].set_ylabel('Residuals')\n",
    "    axes[0, idx].set_title(f'{name} - Residuals vs Predicted')\n",
    "    axes[0, idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual distribution\n",
    "    axes[1, idx].hist(residuals, bins=30, alpha=0.7, density=True)\n",
    "    axes[1, idx].axvline(residuals.mean(), color='r', linestyle='--', \n",
    "                        label=f'Mean: {residuals.mean():.4f}')\n",
    "    axes[1, idx].set_xlabel('Residuals')\n",
    "    axes[1, idx].set_ylabel('Density')\n",
    "    axes[1, idx].set_title(f'{name} - Residual Distribution')\n",
    "    axes[1, idx].legend()\n",
    "    axes[1, idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Model comparison metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# R¬≤ comparison\n",
    "r2_scores = [r2_score(y_test, preds) for _, preds in models]\n",
    "axes[0].bar([name for name, _ in models], r2_scores, color=['blue', 'green', 'red'])\n",
    "axes[0].set_ylabel('R¬≤ Score')\n",
    "axes[0].set_title('R¬≤ Score Comparison')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE comparison\n",
    "rmse_scores = [np.sqrt(mean_squared_error(y_test, preds)) for _, preds in models]\n",
    "axes[1].bar([name for name, _ in models], rmse_scores, color=['blue', 'green', 'red'])\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].set_title('RMSE Comparison')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE comparison\n",
    "mae_scores = [mean_absolute_error(y_test, preds) for _, preds in models]\n",
    "axes[2].bar([name for name, _ in models], mae_scores, color=['blue', 'green', 'red'])\n",
    "axes[2].set_ylabel('MAE')\n",
    "axes[2].set_title('MAE Comparison')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Performance visualizations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de580a",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"=== FEATURE IMPORTANCE ANALYSIS ===\\n\")\n",
    "\n",
    "# Extract feature importance from tree-based models\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': X_test.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "gb_importance = pd.DataFrame({\n",
    "    'feature': X_test.columns,\n",
    "    'importance': gb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 Features - Random Forest:\")\n",
    "print(rf_importance.head(15))\n",
    "\n",
    "print(\"\\nTop 15 Features - Gradient Boosting:\")\n",
    "print(gb_importance.head(15))\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Random Forest importance\n",
    "top_rf = rf_importance.head(15)\n",
    "axes[0].barh(range(len(top_rf)), top_rf['importance'])\n",
    "axes[0].set_yticks(range(len(top_rf)))\n",
    "axes[0].set_yticklabels(top_rf['feature'])\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Random Forest - Top 15 Feature Importance')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient Boosting importance\n",
    "top_gb = gb_importance.head(15)\n",
    "axes[1].barh(range(len(top_gb)), top_gb['importance'])\n",
    "axes[1].set_yticks(range(len(top_gb)))\n",
    "axes[1].set_yticklabels(top_gb['feature'])\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Gradient Boosting - Top 15 Feature Importance')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Combined importance (average)\n",
    "combined_importance = pd.merge(rf_importance, gb_importance, on='feature', suffixes=('_rf', '_gb'))\n",
    "combined_importance['avg_importance'] = (combined_importance['importance_rf'] + \n",
    "                                       combined_importance['importance_gb']) / 2\n",
    "combined_importance = combined_importance.sort_values('avg_importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Features - Combined Importance:\")\n",
    "print(combined_importance[['feature', 'avg_importance']].head(15))\n",
    "\n",
    "# Save feature importance\n",
    "combined_importance.to_csv('../data/model_feature_importance.csv', index=False)\n",
    "print(\"\\nFeature importance saved to ../data/model_feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e0ed9",
   "metadata": {},
   "source": [
    "## 5. Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b42f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series prediction analysis\n",
    "print(\"=== TIME SERIES PREDICTION ANALYSIS ===\\n\")\n",
    "\n",
    "# Create time series plots\n",
    "if hasattr(y_test.index, 'to_datetime'):\n",
    "    time_index = pd.to_datetime(y_test.index)\n",
    "else:\n",
    "    time_index = range(len(y_test))\n",
    "\n",
    "# Plot actual vs predicted over time\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "fig.suptitle('Time Series Predictions vs Actual', fontsize=16)\n",
    "\n",
    "models_ts = [('Random Forest', rf_predictions), ('Gradient Boosting', gb_predictions), \n",
    "             ('Ensemble', ensemble_predictions)]\n",
    "\n",
    "for idx, (name, preds) in enumerate(models_ts):\n",
    "    # Sample data for visualization (plot every 10th point if too many)\n",
    "    step = max(1, len(y_test) // 200)\n",
    "    \n",
    "    axes[idx].plot(time_index[::step], y_test.iloc[::step], 'b-', alpha=0.7, label='Actual', linewidth=1)\n",
    "    axes[idx].plot(time_index[::step], preds[::step], 'r-', alpha=0.7, label='Predicted', linewidth=1)\n",
    "    axes[idx].set_ylabel('Volatility')\n",
    "    axes[idx].set_title(f'{name}')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel('Time')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prediction error over time\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Prediction Errors Over Time', fontsize=16)\n",
    "\n",
    "for idx, (name, preds) in enumerate(models_ts):\n",
    "    row, col = idx // 2, idx % 2\n",
    "    if idx < 3:\n",
    "        errors = y_test - preds\n",
    "        axes[row, col].plot(time_index[::step], errors[::step], alpha=0.7)\n",
    "        axes[row, col].axhline(y=0, color='r', linestyle='--')\n",
    "        axes[row, col].set_ylabel('Prediction Error')\n",
    "        axes[row, col].set_title(f'{name} - Prediction Errors')\n",
    "        axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "if len(models_ts) < 4:\n",
    "    axes[1, 1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Rolling performance metrics\n",
    "window_size = max(20, len(y_test) // 20)\n",
    "rolling_r2 = []\n",
    "rolling_rmse = []\n",
    "\n",
    "for i in range(window_size, len(y_test)):\n",
    "    window_actual = y_test.iloc[i-window_size:i]\n",
    "    window_pred = ensemble_predictions[i-window_size:i]\n",
    "    \n",
    "    r2 = r2_score(window_actual, window_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(window_actual, window_pred))\n",
    "    \n",
    "    rolling_r2.append(r2)\n",
    "    rolling_rmse.append(rmse)\n",
    "\n",
    "# Plot rolling metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(rolling_r2)\n",
    "axes[0].set_title(f'Rolling R¬≤ Score (window={window_size})')\n",
    "axes[0].set_ylabel('R¬≤ Score')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(rolling_rmse)\n",
    "axes[1].set_title(f'Rolling RMSE (window={window_size})')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Rolling performance analysis completed (window size: {window_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5507df71",
   "metadata": {},
   "source": [
    "## 6. Volatility Classification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb0080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility classification analysis\n",
    "print(\"=== VOLATILITY CLASSIFICATION ANALYSIS ===\\n\")\n",
    "\n",
    "# Define volatility regimes\n",
    "def classify_volatility(vol_values):\n",
    "    \"\"\"Classify volatility into regimes.\"\"\"\n",
    "    low_threshold = np.percentile(vol_values, 33)\n",
    "    high_threshold = np.percentile(vol_values, 67)\n",
    "    \n",
    "    conditions = [\n",
    "        vol_values <= low_threshold,\n",
    "        (vol_values > low_threshold) & (vol_values <= high_threshold),\n",
    "        vol_values > high_threshold\n",
    "    ]\n",
    "    choices = ['Low', 'Medium', 'High']\n",
    "    \n",
    "    return np.select(conditions, choices)\n",
    "\n",
    "# Classify actual and predicted volatilities\n",
    "actual_classes = classify_volatility(y_test)\n",
    "ensemble_classes = classify_volatility(ensemble_predictions)\n",
    "\n",
    "# Classification accuracy\n",
    "class_accuracy = (actual_classes == ensemble_classes).mean()\n",
    "print(f\"Volatility regime classification accuracy: {class_accuracy:.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(actual_classes, ensemble_classes, labels=['Low', 'Medium', 'High'])\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(pd.DataFrame(cm, index=['Low', 'Medium', 'High'], columns=['Low', 'Medium', 'High']))\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(actual_classes, ensemble_classes))\n",
    "\n",
    "# Visualize classification performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Low', 'Medium', 'High'],\n",
    "            yticklabels=['Low', 'Medium', 'High'], ax=axes[0])\n",
    "axes[0].set_title('Volatility Classification Confusion Matrix')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "\n",
    "# Class distribution\n",
    "actual_counts = pd.Series(actual_classes).value_counts()\n",
    "pred_counts = pd.Series(ensemble_classes).value_counts()\n",
    "\n",
    "x = np.arange(len(actual_counts))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x - width/2, actual_counts.values, width, label='Actual', alpha=0.7)\n",
    "axes[1].bar(x + width/2, pred_counts.values, width, label='Predicted', alpha=0.7)\n",
    "axes[1].set_xlabel('Volatility Regime')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Volatility Regime Distribution')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(actual_counts.index)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance by volatility regime\n",
    "print(f\"\\nPerformance by Volatility Regime:\")\n",
    "for regime in ['Low', 'Medium', 'High']:\n",
    "    regime_mask = actual_classes == regime\n",
    "    if regime_mask.sum() > 0:\n",
    "        regime_r2 = r2_score(y_test[regime_mask], ensemble_predictions[regime_mask])\n",
    "        regime_rmse = np.sqrt(mean_squared_error(y_test[regime_mask], ensemble_predictions[regime_mask]))\n",
    "        print(f\"  {regime} Volatility: R¬≤ = {regime_r2:.4f}, RMSE = {regime_rmse:.4f} (n={regime_mask.sum()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c6197f",
   "metadata": {},
   "source": [
    "## 7. Business Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6732f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business impact analysis\n",
    "print(\"=== BUSINESS IMPACT ANALYSIS ===\\n\")\n",
    "\n",
    "# Trading strategy simulation\n",
    "def simulate_trading_strategy(actual_vol, predicted_vol, threshold=0.02):\n",
    "    \"\"\"Simulate a volatility-based trading strategy.\"\"\"\n",
    "    \n",
    "    # Strategy: Buy when predicted volatility is low, sell when high\n",
    "    signals = np.where(predicted_vol < threshold, 1,  # Buy signal\n",
    "                      np.where(predicted_vol > threshold * 2, -1, 0))  # Sell signal\n",
    "    \n",
    "    # Simulate returns (assuming inverse relationship with volatility)\n",
    "    # Higher volatility = higher risk = potential for higher returns but more losses\n",
    "    base_returns = np.random.normal(0.001, actual_vol)  # Daily returns based on actual volatility\n",
    "    \n",
    "    # Strategy returns\n",
    "    strategy_returns = signals * base_returns\n",
    "    \n",
    "    # Calculate cumulative returns\n",
    "    cumulative_returns = (1 + strategy_returns).cumprod()\n",
    "    \n",
    "    return {\n",
    "        'total_return': cumulative_returns[-1] - 1,\n",
    "        'sharpe_ratio': strategy_returns.mean() / strategy_returns.std() * np.sqrt(252),\n",
    "        'max_drawdown': (cumulative_returns / cumulative_returns.expanding().max() - 1).min(),\n",
    "        'win_rate': (strategy_returns > 0).mean(),\n",
    "        'num_trades': (signals != 0).sum()\n",
    "    }\n",
    "\n",
    "# Simulate strategies\n",
    "perfect_strategy = simulate_trading_strategy(y_test, y_test)  # Perfect predictions\n",
    "ensemble_strategy = simulate_trading_strategy(y_test, ensemble_predictions)  # Our model\n",
    "random_strategy = simulate_trading_strategy(y_test, np.random.rand(len(y_test)) * y_test.std())  # Random\n",
    "\n",
    "# Results comparison\n",
    "strategy_results = pd.DataFrame({\n",
    "    'Perfect Predictions': perfect_strategy,\n",
    "    'Ensemble Model': ensemble_strategy,\n",
    "    'Random Predictions': random_strategy\n",
    "})\n",
    "\n",
    "print(\"Trading Strategy Simulation Results:\")\n",
    "print(strategy_results.round(4))\n",
    "\n",
    "# Risk assessment accuracy\n",
    "def assess_risk_prediction_accuracy(actual_vol, predicted_vol, risk_threshold=0.03):\n",
    "    \"\"\"Assess accuracy of risk predictions.\"\"\"\n",
    "    \n",
    "    actual_high_risk = actual_vol > risk_threshold\n",
    "    predicted_high_risk = predicted_vol > risk_threshold\n",
    "    \n",
    "    # Calculate risk prediction metrics\n",
    "    tp = ((actual_high_risk) & (predicted_high_risk)).sum()  # True positives\n",
    "    tn = ((~actual_high_risk) & (~predicted_high_risk)).sum()  # True negatives\n",
    "    fp = ((~actual_high_risk) & (predicted_high_risk)).sum()  # False positives\n",
    "    fn = ((actual_high_risk) & (~predicted_high_risk)).sum()  # False negatives\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'true_positives': tp,\n",
    "        'false_positives': fp,\n",
    "        'false_negatives': fn\n",
    "    }\n",
    "\n",
    "risk_assessment = assess_risk_prediction_accuracy(y_test, ensemble_predictions)\n",
    "\n",
    "print(f\"\\nRisk Prediction Assessment:\")\n",
    "for metric, value in risk_assessment.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {metric.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "# Volatility forecasting accuracy by horizon\n",
    "print(f\"\\nVolatility Forecasting Accuracy:\")\n",
    "print(f\"  Mean Absolute Error: {mean_absolute_error(y_test, ensemble_predictions):.6f}\")\n",
    "print(f\"  Mean Absolute Percentage Error: {np.mean(np.abs((y_test - ensemble_predictions) / y_test)) * 100:.2f}%\")\n",
    "print(f\"  Correlation with Actual: {np.corrcoef(y_test, ensemble_predictions)[0,1]:.4f}\")\n",
    "\n",
    "# Economic impact estimation\n",
    "portfolio_value = 1000000  # $1M portfolio\n",
    "risk_reduction = abs(ensemble_strategy['max_drawdown'] - random_strategy['max_drawdown'])\n",
    "potential_savings = portfolio_value * risk_reduction\n",
    "\n",
    "print(f\"\\nEstimated Economic Impact:\")\n",
    "print(f\"  Portfolio Value: ${portfolio_value:,.0f}\")\n",
    "print(f\"  Max Drawdown Reduction: {risk_reduction:.2%}\")\n",
    "print(f\"  Potential Risk Savings: ${potential_savings:,.0f}\")\n",
    "print(f\"  Sharpe Ratio Improvement: {ensemble_strategy['sharpe_ratio'] - random_strategy['sharpe_ratio']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f1d308",
   "metadata": {},
   "source": [
    "## 8. Model Robustness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362a3ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model robustness and stability analysis\n",
    "print(\"=== MODEL ROBUSTNESS ANALYSIS ===\\n\")\n",
    "\n",
    "# Bootstrap confidence intervals\n",
    "def bootstrap_metric(y_true, y_pred, metric_func, n_bootstrap=1000):\n",
    "    \"\"\"Calculate bootstrap confidence intervals for metrics.\"\"\"\n",
    "    bootstrap_scores = []\n",
    "    n_samples = len(y_true)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Bootstrap sample\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        y_true_boot = y_true.iloc[indices] if hasattr(y_true, 'iloc') else y_true[indices]\n",
    "        y_pred_boot = y_pred[indices]\n",
    "        \n",
    "        # Calculate metric\n",
    "        score = metric_func(y_true_boot, y_pred_boot)\n",
    "        bootstrap_scores.append(score)\n",
    "    \n",
    "    return np.array(bootstrap_scores)\n",
    "\n",
    "# Calculate confidence intervals for key metrics\n",
    "print(\"Bootstrap Confidence Intervals (95%):\")\n",
    "\n",
    "# R¬≤ confidence interval\n",
    "r2_bootstrap = bootstrap_metric(y_test, ensemble_predictions, r2_score)\n",
    "r2_ci = np.percentile(r2_bootstrap, [2.5, 97.5])\n",
    "print(f\"  R¬≤ Score: {r2_score(y_test, ensemble_predictions):.4f} [{r2_ci[0]:.4f}, {r2_ci[1]:.4f}]\")\n",
    "\n",
    "# RMSE confidence interval\n",
    "rmse_bootstrap = bootstrap_metric(y_test, ensemble_predictions, \n",
    "                                 lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "rmse_ci = np.percentile(rmse_bootstrap, [2.5, 97.5])\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_test, ensemble_predictions)):.4f} [{rmse_ci[0]:.4f}, {rmse_ci[1]:.4f}]\")\n",
    "\n",
    "# MAE confidence interval\n",
    "mae_bootstrap = bootstrap_metric(y_test, ensemble_predictions, mean_absolute_error)\n",
    "mae_ci = np.percentile(mae_bootstrap, [2.5, 97.5])\n",
    "print(f\"  MAE: {mean_absolute_error(y_test, ensemble_predictions):.4f} [{mae_ci[0]:.4f}, {mae_ci[1]:.4f}]\")\n",
    "\n",
    "# Prediction stability analysis\n",
    "# Add small noise to features and see how predictions change\n",
    "noise_levels = [0.01, 0.05, 0.1, 0.2]\n",
    "stability_results = []\n",
    "\n",
    "original_predictions = ensemble_predictions.copy()\n",
    "\n",
    "for noise_level in noise_levels:\n",
    "    # Add noise to features\n",
    "    X_noisy = X_test + np.random.normal(0, noise_level, X_test.shape)\n",
    "    \n",
    "    # Generate predictions with noisy features\n",
    "    if hasattr(gb_model, 'predict'):\n",
    "        noisy_predictions = gb_model.predict(X_noisy)\n",
    "    else:\n",
    "        noisy_predictions = original_predictions + np.random.normal(0, noise_level * 0.1, len(original_predictions))\n",
    "    \n",
    "    # Calculate stability metric (correlation with original predictions)\n",
    "    stability = np.corrcoef(original_predictions, noisy_predictions)[0, 1]\n",
    "    stability_results.append(stability)\n",
    "\n",
    "print(f\"\\nPrediction Stability Analysis:\")\n",
    "for noise_level, stability in zip(noise_levels, stability_results):\n",
    "    print(f\"  Noise Level {noise_level}: Correlation = {stability:.4f}\")\n",
    "\n",
    "# Visualize stability\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(noise_levels, stability_results, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Noise Level')\n",
    "plt.ylabel('Prediction Stability (Correlation)')\n",
    "plt.title('Model Prediction Stability vs Input Noise')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Outlier impact analysis\n",
    "# Remove extreme outliers and see how performance changes\n",
    "outlier_threshold = np.percentile(y_test, 95)\n",
    "non_outlier_mask = y_test <= outlier_threshold\n",
    "\n",
    "print(f\"\\nOutlier Impact Analysis:\")\n",
    "print(f\"  Original dataset size: {len(y_test)}\")\n",
    "print(f\"  Non-outlier subset size: {non_outlier_mask.sum()}\")\n",
    "\n",
    "# Performance on non-outlier subset\n",
    "non_outlier_r2 = r2_score(y_test[non_outlier_mask], ensemble_predictions[non_outlier_mask])\n",
    "non_outlier_rmse = np.sqrt(mean_squared_error(y_test[non_outlier_mask], ensemble_predictions[non_outlier_mask]))\n",
    "\n",
    "print(f\"  Performance without outliers:\")\n",
    "print(f\"    R¬≤ Score: {non_outlier_r2:.4f} (vs {r2_score(y_test, ensemble_predictions):.4f} with outliers)\")\n",
    "print(f\"    RMSE: {non_outlier_rmse:.4f} (vs {np.sqrt(mean_squared_error(y_test, ensemble_predictions)):.4f} with outliers)\")\n",
    "\n",
    "print(f\"\\nModel robustness analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35148c7f",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d8e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation summary\n",
    "print(\"=== MODEL EVALUATION SUMMARY ===\\n\")\n",
    "\n",
    "# Best model identification\n",
    "best_model = metrics_df.loc[metrics_df['R¬≤'].idxmax()]\n",
    "\n",
    "print(f\"üèÜ BEST PERFORMING MODEL: {best_model['Model']}\")\n",
    "print(f\"   ‚Ä¢ R¬≤ Score: {best_model['R¬≤']:.4f}\")\n",
    "print(f\"   ‚Ä¢ RMSE: {best_model['RMSE']:.4f}\")\n",
    "print(f\"   ‚Ä¢ MAE: {best_model['MAE']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Directional Accuracy: {best_model['Directional Accuracy']:.1f}%\")\n",
    "\n",
    "print(f\"\\nüìä MODEL COMPARISON:\")\n",
    "for _, row in metrics_df.iterrows():\n",
    "    print(f\"   ‚Ä¢ {row['Model']}: R¬≤ = {row['R¬≤']:.4f}, RMSE = {row['RMSE']:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ KEY PERFORMANCE INSIGHTS:\")\n",
    "print(f\"   ‚Ä¢ Ensemble method shows {'best' if best_model['Model'] == 'Ensemble' else 'competitive'} performance\")\n",
    "print(f\"   ‚Ä¢ Volatility classification accuracy: {class_accuracy:.1%}\")\n",
    "print(f\"   ‚Ä¢ Risk prediction precision: {risk_assessment['precision']:.3f}\")\n",
    "print(f\"   ‚Ä¢ Model predictions are {'stable' if min(stability_results) > 0.8 else 'moderately stable'} under noise\")\n",
    "\n",
    "print(f\"\\nüíº BUSINESS VALUE:\")\n",
    "print(f\"   ‚Ä¢ Trading strategy Sharpe ratio: {ensemble_strategy['sharpe_ratio']:.3f}\")\n",
    "print(f\"   ‚Ä¢ Maximum drawdown: {ensemble_strategy['max_drawdown']:.2%}\")\n",
    "print(f\"   ‚Ä¢ Risk reduction potential: ${potential_savings:,.0f} on $1M portfolio\")\n",
    "print(f\"   ‚Ä¢ High-risk event detection recall: {risk_assessment['recall']:.3f}\")\n",
    "\n",
    "print(f\"\\nüîç TOP PREDICTIVE FEATURES:\")\n",
    "top_5_features = combined_importance.head(5)\n",
    "for idx, row in top_5_features.iterrows():\n",
    "    print(f\"   {idx+1}. {row['feature']}: {row['avg_importance']:.4f}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è MODEL LIMITATIONS:\")\n",
    "print(f\"   ‚Ä¢ Performance varies by volatility regime\")\n",
    "print(f\"   ‚Ä¢ Outliers impact prediction accuracy\")\n",
    "print(f\"   ‚Ä¢ Feature noise sensitivity: {1 - min(stability_results):.1%} correlation drop at 20% noise\")\n",
    "print(f\"   ‚Ä¢ Temporal stability needs monitoring in deployment\")\n",
    "\n",
    "print(f\"\\n‚úÖ MODEL READINESS ASSESSMENT:\")\n",
    "readiness_score = 0\n",
    "criteria = {\n",
    "    'R¬≤ Score > 0.7': best_model['R¬≤'] > 0.7,\n",
    "    'Classification Accuracy > 70%': class_accuracy > 0.7,\n",
    "    'Risk Precision > 0.6': risk_assessment['precision'] > 0.6,\n",
    "    'Stability > 0.8': min(stability_results) > 0.8,\n",
    "    'Business Value Positive': ensemble_strategy['sharpe_ratio'] > 0\n",
    "}\n",
    "\n",
    "for criterion, passed in criteria.items():\n",
    "    status = \"‚úÖ\" if passed else \"‚ùå\"\n",
    "    print(f\"   {status} {criterion}\")\n",
    "    if passed:\n",
    "        readiness_score += 1\n",
    "\n",
    "readiness_percentage = (readiness_score / len(criteria)) * 100\n",
    "print(f\"\\nüìà OVERALL READINESS SCORE: {readiness_score}/{len(criteria)} ({readiness_percentage:.0f}%)\")\n",
    "\n",
    "if readiness_percentage >= 80:\n",
    "    recommendation = \"‚úÖ RECOMMENDED FOR PRODUCTION DEPLOYMENT\"\n",
    "elif readiness_percentage >= 60:\n",
    "    recommendation = \"‚ö†Ô∏è REQUIRES IMPROVEMENTS BEFORE DEPLOYMENT\"\n",
    "else:\n",
    "    recommendation = \"‚ùå NOT READY FOR PRODUCTION\"\n",
    "\n",
    "print(f\"\\nüéØ DEPLOYMENT RECOMMENDATION: {recommendation}\")\n",
    "\n",
    "print(f\"\\nüìÅ EVALUATION OUTPUTS:\")\n",
    "print(f\"   ‚Ä¢ Model performance metrics calculated\")\n",
    "print(f\"   ‚Ä¢ Feature importance rankings saved\")\n",
    "print(f\"   ‚Ä¢ Prediction results analyzed\")\n",
    "print(f\"   ‚Ä¢ Business impact assessment completed\")\n",
    "print(f\"   ‚Ä¢ Robustness analysis performed\")\n",
    "\n",
    "# Save evaluation results\n",
    "evaluation_summary = {\n",
    "    'best_model': best_model['Model'],\n",
    "    'performance_metrics': metrics_df.to_dict('records'),\n",
    "    'classification_accuracy': class_accuracy,\n",
    "    'risk_assessment': risk_assessment,\n",
    "    'business_metrics': ensemble_strategy,\n",
    "    'stability_analysis': dict(zip(noise_levels, stability_results)),\n",
    "    'readiness_score': readiness_percentage\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../data/evaluation_summary.json', 'w') as f:\n",
    "    json.dump(evaluation_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nüíæ Evaluation summary saved to ../data/evaluation_summary.json\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ MODEL EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"   Ready for deployment decision and monitoring setup\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
