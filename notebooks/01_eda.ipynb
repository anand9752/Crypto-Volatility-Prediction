{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2129cb",
   "metadata": {},
   "source": [
    "# Cryptocurrency Volatility Prediction - Exploratory Data Analysis\n",
    "\n",
    "## Project Overview\n",
    "This notebook provides a comprehensive exploratory data analysis (EDA) for cryptocurrency volatility prediction. We analyze historical market data including OHLC prices, trading volume, and market capitalization to understand patterns and relationships that can help in building effective volatility prediction models.\n",
    "\n",
    "## Objectives\n",
    "- Understand the structure and quality of the cryptocurrency dataset\n",
    "- Analyze price trends and volatility patterns across different cryptocurrencies\n",
    "- Identify key features that influence volatility\n",
    "- Prepare insights for machine learning model development\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf11b8d",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bc9663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../dataset.csv')\n",
    "\n",
    "# Basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nColumn Names:\", df.columns.tolist())\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nBasic Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d5c177",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a05d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Set timestamp as index for time series analysis\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Sort by timestamp\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# Check unique symbols\n",
    "print(f\"Number of unique cryptocurrencies: {df['symbol'].nunique()}\")\n",
    "print(f\"Cryptocurrencies: {sorted(df['symbol'].unique())}\")\n",
    "\n",
    "# Check time range\n",
    "print(f\"\\nTime range: {df.index.min()} to {df.index.max()}\")\n",
    "print(f\"Total time span: {(df.index.max() - df.index.min()).days} days\")\n",
    "\n",
    "# Check data frequency\n",
    "print(f\"\\nData points per symbol:\")\n",
    "symbol_counts = df.groupby('symbol').size().sort_values(ascending=False)\n",
    "print(symbol_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ca0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Analysis\n",
    "print(\"=== DATA QUALITY ANALYSIS ===\\n\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicates}\")\n",
    "\n",
    "# Check for invalid prices (negative values)\n",
    "negative_prices = (df[['open', 'high', 'low', 'close']] < 0).any(axis=1).sum()\n",
    "print(f\"Rows with negative prices: {negative_prices}\")\n",
    "\n",
    "# Check for logical inconsistencies (high < low, etc.)\n",
    "price_inconsistencies = (df['high'] < df['low']).sum()\n",
    "print(f\"Rows where high < low: {price_inconsistencies}\")\n",
    "\n",
    "# Check for zero volumes\n",
    "zero_volumes = (df['volume'] == 0).sum()\n",
    "print(f\"Rows with zero volume: {zero_volumes}\")\n",
    "\n",
    "# Check for extreme outliers using IQR method\n",
    "def detect_outliers(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = ((data[column] < lower_bound) | (data[column] > upper_bound)).sum()\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "print(f\"\\nOutlier Analysis (using IQR method):\")\n",
    "for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "    outliers, lower, upper = detect_outliers(df, col)\n",
    "    print(f\"{col}: {outliers} outliers (bounds: {lower:.2f} - {upper:.2f})\")\n",
    "\n",
    "# Data completeness by symbol\n",
    "print(f\"\\nData completeness by symbol:\")\n",
    "completeness = df.groupby('symbol').apply(lambda x: (1 - x.isnull().sum() / len(x)).mean()).sort_values(ascending=False)\n",
    "print(completeness.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c654dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price Analysis\n",
    "print(\"=== PRICE ANALYSIS ===\\n\")\n",
    "\n",
    "# Calculate basic price metrics\n",
    "df['daily_return'] = df.groupby('symbol')['close'].pct_change()\n",
    "df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "df['volatility'] = df.groupby('symbol')['daily_return'].rolling(window=20).std().reset_index(0, drop=True)\n",
    "\n",
    "# Price statistics by symbol\n",
    "price_stats = df.groupby('symbol').agg({\n",
    "    'close': ['min', 'max', 'mean', 'std'],\n",
    "    'volume': ['min', 'max', 'mean', 'std'],\n",
    "    'daily_return': ['mean', 'std', 'skew'],\n",
    "    'volatility': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "price_stats.columns = ['_'.join(col) for col in price_stats.columns]\n",
    "print(\"Price Statistics by Symbol (top 10 by avg close price):\")\n",
    "print(price_stats.sort_values('close_mean', ascending=False).head(10))\n",
    "\n",
    "# Correlation analysis\n",
    "numeric_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "print(f\"\\nCorrelation Matrix:\")\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# Return statistics\n",
    "print(f\"\\nReturn Statistics (overall):\")\n",
    "print(f\"Mean daily return: {df['daily_return'].mean():.4f}\")\n",
    "print(f\"Std daily return: {df['daily_return'].std():.4f}\")\n",
    "print(f\"Skewness: {df['daily_return'].skew():.4f}\")\n",
    "print(f\"Kurtosis: {df['daily_return'].kurtosis():.4f}\")\n",
    "\n",
    "# Top volatile cryptocurrencies\n",
    "print(f\"\\nTop 10 Most Volatile Cryptocurrencies:\")\n",
    "avg_volatility = df.groupby('symbol')['volatility'].mean().sort_values(ascending=False)\n",
    "print(avg_volatility.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71005894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market Analysis\n",
    "print(\"=== MARKET ANALYSIS ===\\n\")\n",
    "\n",
    "# Market capitalization analysis (assuming market cap data if available)\n",
    "# For now, we'll use price * volume as a proxy for market activity\n",
    "df['market_activity'] = df['close'] * df['volume']\n",
    "\n",
    "market_activity_stats = df.groupby('symbol')['market_activity'].agg(['sum', 'mean', 'std']).sort_values('sum', ascending=False)\n",
    "print(\"Top 10 Cryptocurrencies by Total Market Activity:\")\n",
    "print(market_activity_stats.head(10))\n",
    "\n",
    "# Temporal patterns\n",
    "df_reset = df.reset_index()\n",
    "df_reset['year'] = df_reset['timestamp'].dt.year\n",
    "df_reset['month'] = df_reset['timestamp'].dt.month\n",
    "df_reset['day_of_week'] = df_reset['timestamp'].dt.dayofweek\n",
    "df_reset['hour'] = df_reset['timestamp'].dt.hour\n",
    "\n",
    "# Yearly trends\n",
    "yearly_stats = df_reset.groupby('year').agg({\n",
    "    'close': 'mean',\n",
    "    'volume': 'mean',\n",
    "    'daily_return': 'mean',\n",
    "    'volatility': 'mean'\n",
    "}).round(4)\n",
    "print(f\"\\nYearly Market Trends:\")\n",
    "print(yearly_stats)\n",
    "\n",
    "# Monthly patterns\n",
    "monthly_returns = df_reset.groupby('month')['daily_return'].mean()\n",
    "print(f\"\\nAverage Returns by Month:\")\n",
    "for month, ret in monthly_returns.items():\n",
    "    print(f\"Month {month}: {ret:.4f}\")\n",
    "\n",
    "# Day of week patterns\n",
    "dow_returns = df_reset.groupby('day_of_week')['daily_return'].mean()\n",
    "dow_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "print(f\"\\nAverage Returns by Day of Week:\")\n",
    "for dow, ret in dow_returns.items():\n",
    "    print(f\"{dow_names[dow]}: {ret:.4f}\")\n",
    "\n",
    "# Volume patterns\n",
    "print(f\"\\nVolume Analysis:\")\n",
    "print(f\"Average daily volume: {df['volume'].mean():.2f}\")\n",
    "print(f\"Median daily volume: {df['volume'].median():.2f}\")\n",
    "print(f\"Max daily volume: {df['volume'].max():.2f}\")\n",
    "\n",
    "# Identify market regime changes (high volatility periods)\n",
    "high_volatility_threshold = df['volatility'].quantile(0.9)\n",
    "high_vol_periods = df[df['volatility'] > high_volatility_threshold]\n",
    "print(f\"\\nHigh Volatility Analysis:\")\n",
    "print(f\"High volatility threshold (90th percentile): {high_volatility_threshold:.4f}\")\n",
    "print(f\"Number of high volatility periods: {len(high_vol_periods)}\")\n",
    "print(f\"Percentage of time in high volatility: {len(high_vol_periods)/len(df)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e182f8dd",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871a9d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "print(\"=== CREATING VISUALIZATIONS ===\\n\")\n",
    "\n",
    "# 1. Price Evolution for Top Cryptocurrencies\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Price Evolution Analysis', fontsize=16)\n",
    "\n",
    "# Get top 5 cryptocurrencies by market activity\n",
    "top_cryptos = market_activity_stats.head(5).index.tolist()\n",
    "\n",
    "# Price evolution\n",
    "for i, symbol in enumerate(top_cryptos[:4]):\n",
    "    ax = axes[i//2, i%2]\n",
    "    symbol_data = df[df['symbol'] == symbol]['close']\n",
    "    ax.plot(symbol_data.index, symbol_data.values, linewidth=1)\n",
    "    ax.set_title(f'{symbol} Price Evolution')\n",
    "    ax.set_ylabel('Price (USD)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Volatility Distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Overall volatility distribution\n",
    "axes[0].hist(df['volatility'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Volatility Distribution (All Cryptocurrencies)')\n",
    "axes[0].set_xlabel('Volatility')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volatility by top cryptocurrencies\n",
    "for symbol in top_cryptos[:5]:\n",
    "    symbol_vol = df[df['symbol'] == symbol]['volatility'].dropna()\n",
    "    axes[1].hist(symbol_vol, bins=30, alpha=0.5, label=symbol)\n",
    "\n",
    "axes[1].set_title('Volatility Distribution by Cryptocurrency')\n",
    "axes[1].set_xlabel('Volatility')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Return Distribution Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Return Distribution Analysis', fontsize=16)\n",
    "\n",
    "# Daily returns histogram\n",
    "axes[0,0].hist(df['daily_return'].dropna(), bins=100, alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_title('Daily Returns Distribution')\n",
    "axes[0,0].set_xlabel('Daily Return')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot for normality check\n",
    "from scipy import stats\n",
    "stats.probplot(df['daily_return'].dropna(), dist=\"norm\", plot=axes[0,1])\n",
    "axes[0,1].set_title('Q-Q Plot: Daily Returns vs Normal Distribution')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot of returns by year\n",
    "df_reset.boxplot(column='daily_return', by='year', ax=axes[1,0])\n",
    "axes[1,0].set_title('Daily Returns by Year')\n",
    "axes[1,0].set_xlabel('Year')\n",
    "axes[1,0].set_ylabel('Daily Return')\n",
    "\n",
    "# Volatility over time\n",
    "monthly_vol = df_reset.groupby(['year', 'month'])['volatility'].mean().reset_index()\n",
    "monthly_vol['date'] = pd.to_datetime(monthly_vol[['year', 'month']].assign(day=1))\n",
    "axes[1,1].plot(monthly_vol['date'], monthly_vol['volatility'])\n",
    "axes[1,1].set_title('Average Monthly Volatility Over Time')\n",
    "axes[1,1].set_xlabel('Date')\n",
    "axes[1,1].set_ylabel('Volatility')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualizations completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b1527d",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis\n",
    "\n",
    "### 4.1 Price Trends and Market Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f27474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis and Advanced Visualizations\n",
    "print(\"=== CORRELATION ANALYSIS ===\\n\")\n",
    "\n",
    "# 4. Correlation Heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Basic price correlations\n",
    "price_corr = df[['open', 'high', 'low', 'close', 'volume']].corr()\n",
    "sns.heatmap(price_corr, annot=True, cmap='coolwarm', center=0, ax=axes[0])\n",
    "axes[0].set_title('Price Variables Correlation Matrix')\n",
    "\n",
    "# Extended correlations including derived features\n",
    "extended_features = ['close', 'volume', 'daily_return', 'volatility', 'market_activity']\n",
    "extended_corr = df[extended_features].corr()\n",
    "sns.heatmap(extended_corr, annot=True, cmap='coolwarm', center=0, ax=axes[1])\n",
    "axes[1].set_title('Extended Features Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Cross-cryptocurrency correlation (for top cryptos)\n",
    "print(\"Cross-cryptocurrency correlation analysis...\")\n",
    "\n",
    "# Create a pivot table for closing prices\n",
    "price_pivot = df.reset_index().pivot_table(\n",
    "    index='timestamp', \n",
    "    columns='symbol', \n",
    "    values='close'\n",
    ")\n",
    "\n",
    "# Select top 10 cryptocurrencies for correlation analysis\n",
    "top_10_cryptos = price_pivot.count().sort_values(ascending=False).head(10).index\n",
    "price_pivot_top = price_pivot[top_10_cryptos]\n",
    "\n",
    "# Calculate returns for correlation\n",
    "returns_pivot = price_pivot_top.pct_change().dropna()\n",
    "\n",
    "# Correlation matrix\n",
    "crypto_corr = returns_pivot.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(crypto_corr, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Cross-Cryptocurrency Return Correlations (Top 10 by Data Availability)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Volume vs Price Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Volume vs Price Analysis', fontsize=16)\n",
    "\n",
    "# Volume vs Price scatter\n",
    "sample_data = df.sample(n=min(10000, len(df)))  # Sample for performance\n",
    "axes[0,0].scatter(sample_data['volume'], sample_data['close'], alpha=0.5)\n",
    "axes[0,0].set_xlabel('Volume')\n",
    "axes[0,0].set_ylabel('Close Price')\n",
    "axes[0,0].set_title('Volume vs Close Price')\n",
    "axes[0,0].set_xscale('log')\n",
    "axes[0,0].set_yscale('log')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volume distribution (log scale)\n",
    "axes[0,1].hist(np.log10(df['volume'][df['volume'] > 0]), bins=50, alpha=0.7)\n",
    "axes[0,1].set_xlabel('Log10(Volume)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].set_title('Volume Distribution (Log Scale)')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Daily return vs Volume\n",
    "axes[1,0].scatter(sample_data['volume'], sample_data['daily_return'], alpha=0.5)\n",
    "axes[1,0].set_xlabel('Volume')\n",
    "axes[1,0].set_ylabel('Daily Return')\n",
    "axes[1,0].set_title('Daily Return vs Volume')\n",
    "axes[1,0].set_xscale('log')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volatility vs Volume\n",
    "sample_vol_data = df[df['volatility'].notna()].sample(n=min(5000, len(df[df['volatility'].notna()])))\n",
    "axes[1,1].scatter(sample_vol_data['volume'], sample_vol_data['volatility'], alpha=0.5)\n",
    "axes[1,1].set_xlabel('Volume')\n",
    "axes[1,1].set_ylabel('Volatility')\n",
    "axes[1,1].set_title('Volatility vs Volume')\n",
    "axes[1,1].set_xscale('log')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Advanced visualizations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad88390e",
   "metadata": {},
   "source": [
    "### 4.2 Volatility Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd02fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Insights and Summary\n",
    "print(\"=== KEY INSIGHTS FROM EXPLORATORY DATA ANALYSIS ===\\n\")\n",
    "\n",
    "# Calculate key summary statistics\n",
    "total_symbols = df['symbol'].nunique()\n",
    "total_records = len(df)\n",
    "date_range = (df.index.max() - df.index.min()).days\n",
    "avg_daily_return = df['daily_return'].mean()\n",
    "overall_volatility = df['volatility'].mean()\n",
    "\n",
    "print(\"📊 DATASET OVERVIEW:\")\n",
    "print(f\"   • Total cryptocurrencies: {total_symbols}\")\n",
    "print(f\"   • Total data points: {total_records:,}\")\n",
    "print(f\"   • Time span: {date_range} days ({date_range/365.25:.1f} years)\")\n",
    "print(f\"   • Average records per symbol: {total_records/total_symbols:.0f}\")\n",
    "\n",
    "print(f\"\\n📈 MARKET PERFORMANCE:\")\n",
    "print(f\"   • Average daily return: {avg_daily_return:.4f} ({avg_daily_return*100:.2f}%)\")\n",
    "print(f\"   • Average volatility: {overall_volatility:.4f}\")\n",
    "print(f\"   • Return skewness: {df['daily_return'].skew():.3f}\")\n",
    "print(f\"   • Return kurtosis: {df['daily_return'].kurtosis():.3f}\")\n",
    "\n",
    "# Most and least volatile cryptocurrencies\n",
    "most_volatile = avg_volatility.head(3)\n",
    "least_volatile = avg_volatility.tail(3)\n",
    "\n",
    "print(f\"\\n🔥 MOST VOLATILE CRYPTOCURRENCIES:\")\n",
    "for i, (symbol, vol) in enumerate(most_volatile.items(), 1):\n",
    "    print(f\"   {i}. {symbol}: {vol:.4f}\")\n",
    "\n",
    "print(f\"\\n🔒 LEAST VOLATILE CRYPTOCURRENCIES:\")\n",
    "for i, (symbol, vol) in enumerate(least_volatile.items(), 1):\n",
    "    print(f\"   {i}. {symbol}: {vol:.4f}\")\n",
    "\n",
    "# Market concentration\n",
    "top_5_activity = market_activity_stats.head(5)\n",
    "total_activity = market_activity_stats['sum'].sum()\n",
    "top_5_share = top_5_activity['sum'].sum() / total_activity * 100\n",
    "\n",
    "print(f\"\\n🏆 MARKET CONCENTRATION:\")\n",
    "print(f\"   • Top 5 cryptocurrencies account for {top_5_share:.1f}% of total market activity\")\n",
    "print(f\"   • Most active cryptocurrency: {top_5_activity.index[0]}\")\n",
    "\n",
    "# Data quality insights\n",
    "missing_percentage = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100\n",
    "print(f\"\\n✅ DATA QUALITY:\")\n",
    "print(f\"   • Overall data completeness: {100-missing_percentage:.2f}%\")\n",
    "print(f\"   • Duplicate records: {duplicates}\")\n",
    "print(f\"   • Price inconsistencies: {price_inconsistencies}\")\n",
    "\n",
    "# Temporal patterns\n",
    "best_performing_month = monthly_returns.idxmax()\n",
    "worst_performing_month = monthly_returns.idxmin()\n",
    "best_performing_dow = dow_returns.idxmax()\n",
    "\n",
    "print(f\"\\n📅 TEMPORAL PATTERNS:\")\n",
    "print(f\"   • Best performing month: {best_performing_month} ({monthly_returns[best_performing_month]:.4f})\")\n",
    "print(f\"   • Worst performing month: {worst_performing_month} ({monthly_returns[worst_performing_month]:.4f})\")\n",
    "print(f\"   • Best performing day: {dow_names[best_performing_dow]} ({dow_returns[best_performing_dow]:.4f})\")\n",
    "\n",
    "# Cross-correlations insight\n",
    "if 'crypto_corr' in locals():\n",
    "    avg_correlation = crypto_corr.values[np.triu_indices_from(crypto_corr.values, k=1)].mean()\n",
    "    max_correlation = crypto_corr.values[np.triu_indices_from(crypto_corr.values, k=1)].max()\n",
    "    print(f\"\\n🔗 CROSS-CORRELATIONS:\")\n",
    "    print(f\"   • Average pairwise correlation: {avg_correlation:.3f}\")\n",
    "    print(f\"   • Maximum pairwise correlation: {max_correlation:.3f}\")\n",
    "\n",
    "print(f\"\\n🎯 MODELING RECOMMENDATIONS:\")\n",
    "print(f\"   • Focus on volatility prediction given high variability\")\n",
    "print(f\"   • Consider regime-switching models for different market conditions\")\n",
    "print(f\"   • Include volume and market activity as key features\")\n",
    "print(f\"   • Account for temporal patterns in feature engineering\")\n",
    "print(f\"   • Use robust methods due to heavy-tailed return distributions\")\n",
    "print(f\"   • Consider ensemble methods to capture complex relationships\")\n",
    "\n",
    "print(f\"\\n💡 KEY FINDINGS:\")\n",
    "print(f\"   • Cryptocurrency returns show significant non-normal characteristics\")\n",
    "print(f\"   • High volatility clustering suggests GARCH-type modeling potential\")\n",
    "print(f\"   • Strong cross-correlations indicate systematic risk factors\")\n",
    "print(f\"   • Volume patterns correlate with price movements and volatility\")\n",
    "print(f\"   • Temporal patterns exist but are relatively weak\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"📋 EDA ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"   Next steps: Proceed to data preprocessing and feature engineering\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84bcca",
   "metadata": {},
   "source": [
    "### 4.3 Correlation Analysis and Key Insights"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
